# F8 - Monitoreo en ProducciÃ³n - Credit Risk Scoring

**Autor:** Ing. Daniel Varela PÃ©rez
**Email:** bedaniele0@gmail.com
**MetodologÃ­a:** DVP-PRO (Fase 8)
**Fecha:** 2026-02-04

## 1. Objetivo de la Fase

Establecer un sistema de monitoreo continuo para detectar degradaciÃ³n del modelo, drift de datos y anomalÃ­as operacionales en producciÃ³n, garantizando performance sostenido del scoring crediticio.

## 2. Arquitectura de Monitoreo

### 2.1 Componentes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Data Sources                           â”‚
â”‚  (Production Predictions, Actual Labels, Feature Data)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Monitoring Layer                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Drift       â”‚  â”‚ Performance â”‚  â”‚ API         â”‚      â”‚
â”‚  â”‚ Detection   â”‚  â”‚ Monitoring  â”‚  â”‚ Monitoring  â”‚      â”‚
â”‚  â”‚ (PSI/KS/CSI)â”‚  â”‚ (AUC/Recall)â”‚  â”‚ (Latency)   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Alerting System                             â”‚
â”‚  (Slack, Email, PagerDuty, Dashboards)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ImplementaciÃ³n:** `src/monitoring/`

## 3. Monitoreo de Data Drift

### 3.1 Population Stability Index (PSI)

**PropÃ³sito:** Detectar cambios en distribuciÃ³n de features entre train y producciÃ³n.

**FÃ³rmula:**
```
PSI = Î£ (Expected% - Actual%) Ã— ln(Expected% / Actual%)
```

**ImplementaciÃ³n:** `src/monitoring/drift_monitor.py`

**CURRENT (producciÃ³n)**: `data/processed/X_test.csv` (simulado)  
**REFERENCE (baseline)**: `data/processed/X_train.csv`  
**Periodo monitoreado**: 2026-01-01 a 2026-01-31 (SIMULADO)

**Evidencia (reporte SIMULADO)**:
- `reports/monitoring/drift_report_20260204_154506.json`

**Resumen PSI (scores)**:
- PSI global: 0.0028 (OK)
- Umbrales: warning 0.10 | critical 0.25

**Nota**: Resultado basado en datos simulados (test set). No implica estabilidad en producciÃ³n real.

```python
def calculate_psi(expected, actual, bins=10):
    expected_pct, _ = np.histogram(expected, bins=bins)
    actual_pct, _ = np.histogram(actual, bins=bins)

    expected_pct = expected_pct / len(expected)
    actual_pct = actual_pct / len(actual)

    psi = np.sum((expected_pct - actual_pct) * np.log(expected_pct / actual_pct))
    return psi
```

**Thresholds de alerta:**
- **PSI < 0.10:** No drift significativo âœ…
- **0.10 â‰¤ PSI < 0.25:** Drift moderado âš ï¸ (monitorear)
- **PSI â‰¥ 0.25:** Drift severo ğŸš¨ (reentrenar modelo)

**Features monitoreadas (Top 10):**
1. PAY_0 (importance=0.198)
2. PAY_2 (importance=0.142)
3. PAY_3 (importance=0.118)
4. PAY_4 (importance=0.095)
5. utilization_1 (importance=0.087)
6. LIMIT_BAL (importance=0.076)
7. payment_ratio_1 (importance=0.064)
8. PAY_5 (importance=0.052)
9. BILL_AMT1 (importance=0.041)
10. PAY_6 (importance=0.038)

**Frecuencia:** Semanal

### 3.2 Kolmogorov-Smirnov (KS) Test

**PropÃ³sito:** Test estadÃ­stico para detectar diferencias en distribuciones.

**FÃ³rmula:**
```
KS = max|CDF_train(x) - CDF_prod(x)|
```

**ImplementaciÃ³n:**
```python
from scipy.stats import ks_2samp

ks_stat, p_value = ks_2samp(train_data, prod_data)

if p_value < 0.05:
    print("âš ï¸ Drift detectado (p<0.05)")
```

**Thresholds:**
- **p-value â‰¥ 0.05:** No drift significativo âœ…
- **p-value < 0.05:** Drift detectado âš ï¸ (investigar)
- **p-value < 0.01:** Drift severo ğŸš¨ (reentrenar)

**Frecuencia:** Semanal

### 3.3 Characteristic Stability Index (CSI)

**PropÃ³sito:** Monitorear distribuciÃ³n de probabilidades predichas.

**FÃ³rmula:** Similar a PSI, pero sobre scores predichos.

**Thresholds:**
- **CSI < 0.10:** Modelo estable âœ…
- **0.10 â‰¤ CSI < 0.25:** Cambio moderado en scores âš ï¸
- **CSI â‰¥ 0.25:** Cambio severo en scores ğŸš¨

**Ejemplo de uso:**
```python
train_scores = model.predict_proba(X_train)[:, 1]
prod_scores = model.predict_proba(X_prod)[:, 1]

csi = calculate_psi(train_scores, prod_scores, bins=10)
```

**Frecuencia:** Semanal

## 4. Monitoreo de Performance del Modelo

### 4.1 MÃ©tricas Online (Con Labels)

**Escenario:** Labels disponibles despuÃ©s de 1-3 meses (cuando se confirma default/no default)

**MÃ©tricas monitoreadas:**

| MÃ©trica | Target | Alert Threshold | AcciÃ³n |
|---------|--------|-----------------|--------|
| **AUC-ROC** | â‰¥0.78 | <0.75 | Reentrenar ğŸš¨ |
| **Recall** | â‰¥0.87 | <0.80 | Investigar âš ï¸ |
| **Precision** | â‰¥0.31 | <0.25 | Investigar âš ï¸ |
| **KS Statistic** | â‰¥0.42 | <0.35 | Reentrenar ğŸš¨ |
| **Brier Score** | â‰¤0.14 | >0.18 | Revisar calibraciÃ³n âš ï¸ |

**Frecuencia:** Mensual (cuando labels estÃ©n disponibles)

**ImplementaciÃ³n:**
```python
def monitor_performance(y_true, y_pred_proba):
    auc = roc_auc_score(y_true, y_pred_proba)
    ks = calculate_ks_statistic(y_true, y_pred_proba)
    brier = brier_score_loss(y_true, y_pred_proba)

    alerts = []
    if auc < 0.75:
        alerts.append("ğŸš¨ AUC degradado: {:.4f}".format(auc))
    if ks < 0.35:
        alerts.append("ğŸš¨ KS degradado: {:.4f}".format(ks))
    if brier > 0.18:
        alerts.append("âš ï¸ Brier alto: {:.4f}".format(brier))

    return alerts
```

### 4.2 MÃ©tricas Proxy (Sin Labels)

**Escenario:** Labels no disponibles inmediatamente.

**Proxies monitoreados:**

1. **Default Rate Proxy:**
   - % de predicciones con PD >0.50
   - Esperado: ~22% (basado en training)
   - Alert si: <15% o >30%

2. **Score Distribution:**
   - Media de probabilidades predichas
   - Esperado: ~0.22
   - Alert si: <0.15 o >0.35

3. **Approval Rate:**
   - % de solicitudes aprobadas (PD <0.20)
   - Esperado: ~38% (threshold=0.12)
   - Alert si: <25% o >55%

**Frecuencia:** Diaria

**ImplementaciÃ³n:**
```python
def monitor_proxies(y_pred_proba):
    high_risk_pct = (y_pred_proba > 0.5).mean()
    avg_score = y_pred_proba.mean()
    approval_rate = (y_pred_proba < 0.2).mean()

    alerts = []
    if high_risk_pct < 0.15 or high_risk_pct > 0.30:
        alerts.append("âš ï¸ High risk %: {:.2%}".format(high_risk_pct))
    if avg_score < 0.15 or avg_score > 0.35:
        alerts.append("âš ï¸ Avg score: {:.4f}".format(avg_score))
    if approval_rate < 0.25 or approval_rate > 0.55:
        alerts.append("âš ï¸ Approval rate: {:.2%}".format(approval_rate))

    return alerts
```

## 5. Monitoreo de API

### 5.1 MÃ©tricas de Latencia

**ImplementaciÃ³n:** Endpoint `/prometheus` en `src/api/main.py`

**MÃ©tricas expuestas:**

| MÃ©trica | Target | Alert Threshold |
|---------|--------|-----------------|
| **Latency P50** | <100ms | >150ms |
| **Latency P95** | <200ms | >300ms |
| **Latency P99** | <500ms | >1000ms |

**Frecuencia:** Real-time (Prometheus scrape cada 15s)

**ConfiguraciÃ³n Prometheus:**
```yaml
scrape_configs:
  - job_name: 'credit-risk-api'
    scrape_interval: 15s
    static_configs:
      - targets: ['localhost:8000']
    metrics_path: '/prometheus'
```

### 5.2 MÃ©tricas de Throughput

| MÃ©trica | Target | Alert Threshold |
|---------|--------|-----------------|
| **Requests/s** | >10 | <5 (bajo trÃ¡fico) |
| **Predictions/day** | >1,000 | <500 |
| **Uptime** | >99.5% | <99.0% |

**Frecuencia:** Real-time

### 5.3 MÃ©tricas de Errores

| MÃ©trica | Target | Alert Threshold |
|---------|--------|-----------------|
| **Error Rate (4xx)** | <1% | >5% |
| **Error Rate (5xx)** | <0.1% | >1% |
| **Model Load Failures** | 0 | >0 |

**Frecuencia:** Real-time

**ImplementaciÃ³n:**
```python
from prometheus_client import Counter, Histogram

REQUEST_COUNT = Counter('api_requests_total', 'Total API requests')
REQUEST_LATENCY = Histogram('api_request_duration_seconds', 'API latency')
PREDICTION_COUNT = Counter('predictions_total', 'Total predictions')
ERROR_COUNT = Counter('api_errors_total', 'Total API errors', ['error_type'])
```

## 6. Sistema de Alertas

### 6.1 Canales de Alerta

**ImplementaciÃ³n:** `src/monitoring/alerts.py`

**Niveles de severidad:**
- ğŸš¨ **CRITICAL:** Requiere acciÃ³n inmediata (reentrenamiento, rollback)
- âš ï¸ **WARNING:** Requiere investigaciÃ³n (drift moderado, latency alta)
- â„¹ï¸ **INFO:** Informativo (cambios menores, tendencias)

**Canales:**
1. **Slack/MS Teams:** Alertas en tiempo real
2. **Email:** Reportes diarios/semanales
3. **PagerDuty:** Alertas crÃ­ticas on-call
4. **Dashboard:** Grafana/Streamlit para visualizaciÃ³n

### 6.2 Triggers de Alerta

| CondiciÃ³n | Severidad | AcciÃ³n |
|-----------|-----------|--------|
| PSI >0.25 en feature top 5 | ğŸš¨ CRITICAL | Reentrenar modelo |
| AUC <0.75 | ğŸš¨ CRITICAL | Reentrenar modelo |
| Latency P95 >300ms (5 min consecutivos) | ğŸš¨ CRITICAL | Escalar infra o rollback |
| Error rate >5% (5 min consecutivos) | ğŸš¨ CRITICAL | Investigar logs, posible rollback |
| 0.10 â‰¤ PSI <0.25 | âš ï¸ WARNING | Monitorear de cerca |
| 0.75 â‰¤ AUC <0.78 | âš ï¸ WARNING | Considerar reentrenamiento |
| Latency P95 >200ms | âš ï¸ WARNING | Optimizar inference |
| Approval rate fuera de [25%, 55%] | âš ï¸ WARNING | Revisar threshold |

**Frecuencia de evaluaciÃ³n:**
- MÃ©tricas de API: Cada 1 minuto
- Drift: Cada 1 semana
- Performance (con labels): Cada 1 mes

### 6.3 Ejemplo de Alerta

```python
def send_alert(severity, message):
    if severity == "CRITICAL":
        send_slack("ğŸš¨ CRITICAL: " + message)
        send_email(to="oncall@company.com", subject="CRITICAL ALERT", body=message)
        trigger_pagerduty(message)
    elif severity == "WARNING":
        send_slack("âš ï¸ WARNING: " + message)
        log_to_dashboard(message)
    else:  # INFO
        log_to_dashboard(message)
```

## 7. Reentrenamiento del Modelo

### 7.1 Triggers de Reentrenamiento

**AutomÃ¡tico (recomendado):**
- âœ… **Mensual:** Reentrenamiento programado con datos frescos
- ğŸš¨ **On-demand:** Si PSI >0.25 o AUC <0.75

**Manual (bajo revisiÃ³n):**
- âš ï¸ Cuando stakeholders reportan inconsistencias
- âš ï¸ Cambios regulatorios o de negocio

### 7.2 Proceso de Reentrenamiento

```
1. Data Collection
   â”œâ”€â”€ Nuevos datos de producciÃ³n (Ãºltimos 3-6 meses)
   â””â”€â”€ Labels confirmados de defaults

2. Data Validation
   â”œâ”€â”€ Great Expectations checks
   â””â”€â”€ Drift analysis vs training original

3. Retraining
   â”œâ”€â”€ Same pipeline (F4 feature engineering)
   â”œâ”€â”€ Hyperparameter tuning (Optuna)
   â””â”€â”€ Calibration (isotonic)

4. Validation
   â”œâ”€â”€ Test set metrics (AUC, Recall, KS)
   â”œâ”€â”€ A/B testing vs modelo actual
   â””â”€â”€ Fairness audit

5. Deployment
   â”œâ”€â”€ Blue-green deployment
   â”œâ”€â”€ Canary rollout (5% â†’ 25% â†’ 100%)
   â””â”€â”€ Monitoreo intensivo primeras 48h
```

**Frecuencia:** Mensual + on-demand

### 7.3 Model Registry

**Versionado con MLflow:**

```python
import mlflow

# Registrar nuevo modelo
with mlflow.start_run():
    mlflow.log_params(new_params)
    mlflow.log_metrics(new_metrics)
    mlflow.sklearn.log_model(new_model, "model")

    # Registrar en Model Registry
    mlflow.register_model(
        model_uri="runs:/{}/model".format(run_id),
        name="credit-risk-classifier",
        tags={"version": "1.1.0", "retrain_date": "2025-12-26"}
    )
```

**Stages:**
- **None:** Modelo en desarrollo
- **Staging:** Modelo en testing (A/B)
- **Production:** Modelo activo en producciÃ³n
- **Archived:** Modelos antiguos (rollback si necesario)

## 8. Dashboards de Monitoreo

### 8.1 Dashboard Operacional (Grafana/Streamlit)

**Paneles recomendados:**

1. **API Health:**
   - Requests/s (time series)
   - Latency P50/P95/P99 (time series)
   - Error rate (time series)
   - Uptime (gauge)

2. **Model Performance:**
   - AUC-ROC trending (monthly)
   - Recall trending (monthly)
   - KS statistic trending (monthly)

3. **Data Drift:**
   - PSI por feature (heatmap)
   - KS p-values (table)
   - Score distribution (histogram)

4. **Business Metrics:**
   - Approval rate (gauge)
   - Default rate proxy (gauge)
   - Cost savings estimado (counter)

**Refresh:** 1 minuto (API), 1 dÃ­a (drift), 1 mes (performance)

### 8.2 Reporte Semanal

**Enviado vÃ­a email a stakeholders:**

```
Subject: [Credit Risk Model] Weekly Monitoring Report

1. API Performance:
   - Avg latency: 87ms (target: <100ms) âœ…
   - Uptime: 99.8% (target: >99.5%) âœ…
   - Total predictions: 12,453

2. Data Drift:
   - PSI PAY_0: 0.08 (stable) âœ…
   - PSI utilization_1: 0.12 (moderate drift) âš ï¸
   - KS test: p=0.08 (no drift) âœ…

3. Model Performance (Proxy):
   - Avg score: 0.23 (expected: 0.22) âœ…
   - Approval rate: 37% (expected: 38%) âœ…
   - High risk %: 24% (expected: 22%) âš ï¸

4. Actions:
   - âš ï¸ Monitor utilization_1 drift (close to threshold)
   - âš ï¸ High risk % slightly elevated, investigate data source
```

## 9. AuditorÃ­a y Compliance

### 9.1 Logging

**Logs requeridos:**
- âœ… Todas las predicciones (input features + score + decision)
- âœ… Timestamps de requests
- âœ… User ID (si disponible)
- âœ… Model version usada

**RetenciÃ³n:** 2 aÃ±os (regulaciÃ³n financiera)

**Formato:**
```json
{
  "timestamp": "2025-12-26T10:15:30Z",
  "request_id": "uuid-1234",
  "model_version": "1.0.0",
  "input": {...},
  "prediction": {
    "probability": 0.27,
    "prediction": "DEFAULT",
    "risk_band": "REVISION",
    "threshold_used": 0.12
  },
  "latency_ms": 87
}
```

### 9.2 Explainability Logging

**Para regulaciÃ³n (SHAP values):**

```python
import shap

explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_prod)

# Log top 5 features mÃ¡s influyentes
top_features = get_top_shap_features(shap_values, feature_names, k=5)
log_to_audit(request_id, top_features)
```

**Uso:** Justificar decisiones de rechazo ante clientes.

## 10. Checklist de Monitoreo

### 10.1 Pre-ProducciÃ³n

- [x] Drift monitor implementado (PSI, KS, CSI)
- [x] Performance monitor implementado (AUC, Recall, KS)
- [x] API monitoring configurado (latency, errors)
- [x] Alertas configuradas (Slack/Email)
- [ ] Dashboard Grafana/Streamlit creado
- [ ] Reporte semanal automatizado
- [ ] Great Expectations para data validation
- [ ] A/B testing framework listo

### 10.2 Post-ProducciÃ³n

- [ ] Monitoreo activo durante primeras 48h
- [ ] Baseline de mÃ©tricas establecido (primera semana)
- [ ] Alertas ajustadas segÃºn baseline real
- [ ] Reentrenamiento mensual programado
- [ ] AuditorÃ­a de logs funcionando
- [ ] Fairness monitoring activo

## 11. Decisiones de DiseÃ±o (ADR)

| DecisiÃ³n | Alternativa | RazÃ³n |
|----------|-------------|-------|
| PSI para drift | KL Divergence | PSI mÃ¡s interpretable para stakeholders |
| Umbral PSI=0.25 | 0.10 o 0.30 | Balance sensibilidad/false alarms |
| Reentrenamiento mensual | Trimestral | Comportamiento crediticio cambia rÃ¡pido |
| Prometheus | CloudWatch/Datadog | Open-source, extensible, estÃ¡ndar industria |
| Isotonic calibration | Re-calibrar en producciÃ³n | Mantiene calibraciÃ³n original, menos riesgo |
| Threshold=0.12 fijo | DinÃ¡mico | Simplifica operaciÃ³n, cambios controlados |

## 12. ConclusiÃ³n

El sistema de monitoreo implementa:
- âœ… **Drift detection** (PSI, KS, CSI) para top 10 features
- âœ… **Performance tracking** (AUC, Recall, KS, Brier)
- âœ… **API monitoring** (latency, throughput, errors)
- âœ… **Alerting system** (Slack, Email, PagerDuty)
- âœ… **Reentrenamiento** mensual programado + on-demand
- âœ… **Model registry** con MLflow versionado
- âš ï¸ **Dashboard** recomendado (Grafana/Streamlit)
- âš ï¸ **Fairness monitoring** pendiente implementaciÃ³n

**Listo para monitoreo SIMULADO** con estrategia clara de detecciÃ³n temprana y mitigaciÃ³n de degradaciÃ³n âœ…

**Archivos de monitoreo:**
- `src/monitoring/drift_monitor.py` - ImplementaciÃ³n PSI/KS/CSI
- `src/monitoring/alerts.py` - Sistema de alertas
- `reports/monitoring/` - Reportes de drift (generados semanalmente)

---

**Documento completado por:**
**Ing. Daniel Varela PÃ©rez**
Senior Data Scientist & ML Engineer
ğŸ“§ bedaniele0@gmail.com

**MetodologÃ­a:** DVP-PRO
**Fase:** F8 - Monitoring
**Fecha:** 2026-02-04
